2021-05-26 17:50:46 - __main__ - INFO: ===== import configuration =====
2021-05-26 17:50:46 - root - INFO: Config:
2021-05-26 17:50:46 - root - INFO: [Dataset]
2021-05-26 17:50:46 - root - INFO: Dataset_path = data/traditional_ch/msra
2021-05-26 17:50:46 - root - INFO: is_cls_flag = True
2021-05-26 17:50:46 - root - INFO: align_for_token_maxlen = False
2021-05-26 17:50:46 - root - INFO: cls_padding_idx = 0
2021-05-26 17:50:46 - root - INFO: token_padding_idx = 0
2021-05-26 17:50:46 - root - INFO: label_padding_idx = -1
2021-05-26 17:50:46 - root - INFO: token_max_len = 128
2021-05-26 17:50:46 - root - INFO: sample_seed = 207
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [model]
2021-05-26 17:50:46 - root - INFO: mode = training
2021-05-26 17:50:46 - root - INFO: num_labels = 7
2021-05-26 17:50:46 - root - INFO: selected_model = NER_For_Bert
2021-05-26 17:50:46 - root - INFO: selected_optimizer = Transformer_Adaw
2021-05-26 17:50:46 - root - INFO: selected_scheduler = cosine_schedule_with_warmup
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [Label_mapping]
2021-05-26 17:50:46 - root - INFO: O = 0
2021-05-26 17:50:46 - root - INFO: B-PER = 1
2021-05-26 17:50:46 - root - INFO: I-PER = 2
2021-05-26 17:50:46 - root - INFO: B-ORG = 3
2021-05-26 17:50:46 - root - INFO: I-ORG = 4
2021-05-26 17:50:46 - root - INFO: B-LOC = 5
2021-05-26 17:50:46 - root - INFO: I-LOC = 6
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [Label]
2021-05-26 17:50:46 - root - INFO: label_type = PER,ORG,LOC
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [TF_model]
2021-05-26 17:50:46 - root - INFO: TF_Bert_pretrained_model = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/
2021-05-26 17:50:46 - root - INFO: TF_Bert_config = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_config.json
2021-05-26 17:50:46 - root - INFO: TF_Bert_checkpoint = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_model.ckpt
2021-05-26 17:50:46 - root - INFO: TF_Bert_vocab = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/vocab.txt
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [pytorch_model]
2021-05-26 17:50:46 - root - INFO: pytorch_Bert_pretrained_model = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/
2021-05-26 17:50:46 - root - INFO: pytorch_Bert_vocab = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/vocab.txt
2021-05-26 17:50:46 - root - INFO: pytorch_Bert_model = ./experiments/checkpoint_2021-05-25_07_45_37/
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [env]
2021-05-26 17:50:46 - root - INFO: devive = cuda
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [hyperparameter]
2021-05-26 17:50:46 - root - INFO: full_fine_tuning = True
2021-05-26 17:50:46 - root - INFO: batch_size = 16
2021-05-26 17:50:46 - root - INFO: epoch_num = 50
2021-05-26 17:50:46 - root - INFO: learning_rate = 3e-5
2021-05-26 17:50:46 - root - INFO: weight_decay = 0.01
2021-05-26 17:50:46 - root - INFO: clip_grad = 5
2021-05-26 17:50:46 - root - INFO: patience_num = 5
2021-05-26 17:50:46 - root - INFO: patience_value = 0.01
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - root - INFO: [training_setting]
2021-05-26 17:50:46 - root - INFO: saving_model_path = ./experiments/
2021-05-26 17:50:46 - root - INFO: 
2021-05-26 17:50:46 - __main__ - INFO: ===== import data =====
2021-05-26 17:51:11 - __main__ - INFO: ===== getting model =====
2021-05-26 17:51:16 - __main__ - INFO: ===== Starting Training =====
2021-05-26 17:51:16 - __main__ - INFO: ===== Epoch 1=====
2021-05-26 18:23:58 - __main__ - INFO: Epoch: 1, train loss: 329.5366188180106
2021-05-26 18:24:59 - __main__ - INFO: evaluate: {'full': 0.39966666666666667, 'label_score': {'PER': 0.7363333333333333, 'ORG': 0.7096666666666667, 'LOC': 0.521}}
2021-05-26 18:24:59 - __main__ - INFO: best_eval_metric = 0.39966666666666667;current_num_patience = 0
2021-05-26 18:24:59 - __main__ - INFO: ===== Epoch 2=====
2021-05-26 18:57:41 - __main__ - INFO: Epoch: 2, train loss: 136.32379143996465
2021-05-26 18:58:42 - __main__ - INFO: evaluate: {'full': 0.543, 'label_score': {'PER': 0.845, 'ORG': 0.785, 'LOC': 0.6936666666666667}}
2021-05-26 18:58:42 - __main__ - INFO: ***** Saving model *****
2021-05-26 18:58:44 - __main__ - INFO: best_eval_metric = 0.543;current_num_patience = 0
2021-05-26 18:58:44 - __main__ - INFO: ===== Epoch 3=====
2021-05-26 19:31:37 - __main__ - INFO: Epoch: 3, train loss: 84.09923898460751
2021-05-26 19:32:40 - __main__ - INFO: evaluate: {'full': 0.6283333333333333, 'label_score': {'PER': 0.8943333333333333, 'ORG': 0.823, 'LOC': 0.7606666666666668}}
2021-05-26 19:32:40 - __main__ - INFO: ***** Saving model *****
2021-05-26 19:32:42 - __main__ - INFO: best_eval_metric = 0.6283333333333333;current_num_patience = 0
2021-05-26 19:32:42 - __main__ - INFO: ===== Epoch 4=====
2021-05-26 20:05:56 - __main__ - INFO: Epoch: 4, train loss: 58.13240413411458
2021-05-26 20:06:58 - __main__ - INFO: evaluate: {'full': 0.6786666666666666, 'label_score': {'PER': 0.895, 'ORG': 0.843, 'LOC': 0.8213333333333332}}
2021-05-26 20:06:58 - __main__ - INFO: ***** Saving model *****
2021-05-26 20:07:00 - __main__ - INFO: best_eval_metric = 0.6786666666666666;current_num_patience = 0
2021-05-26 20:07:00 - __main__ - INFO: ===== Epoch 5=====
2021-05-26 20:41:27 - __main__ - INFO: Epoch: 5, train loss: 41.85017606172107
2021-05-26 20:42:31 - __main__ - INFO: evaluate: {'full': 0.7296666666666667, 'label_score': {'PER': 0.9303333333333333, 'ORG': 0.855, 'LOC': 0.851}}
2021-05-26 20:42:31 - __main__ - INFO: ***** Saving model *****
2021-05-26 20:42:35 - __main__ - INFO: best_eval_metric = 0.7296666666666667;current_num_patience = 0
2021-05-26 20:42:35 - __main__ - INFO: ===== Epoch 6=====
