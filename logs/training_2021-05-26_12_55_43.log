2021-05-26 12:55:43 - __main__ - INFO: ===== import configuration =====
2021-05-26 12:55:43 - root - INFO: Config:
2021-05-26 12:55:43 - root - INFO: [Dataset]
2021-05-26 12:55:43 - root - INFO: Dataset_path = data/traditional_ch/msra
2021-05-26 12:55:43 - root - INFO: is_cls_flag = True
2021-05-26 12:55:43 - root - INFO: align_for_token_maxlen = False
2021-05-26 12:55:43 - root - INFO: cls_padding_idx = 0
2021-05-26 12:55:43 - root - INFO: token_padding_idx = 0
2021-05-26 12:55:43 - root - INFO: label_padding_idx = -1
2021-05-26 12:55:43 - root - INFO: token_max_len = 128
2021-05-26 12:55:43 - root - INFO: sample_seed = 207
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [model]
2021-05-26 12:55:43 - root - INFO: mode = training
2021-05-26 12:55:43 - root - INFO: num_labels = 7
2021-05-26 12:55:43 - root - INFO: selected_model = NER_For_Bert
2021-05-26 12:55:43 - root - INFO: selected_optimizer = Transformer_Adaw
2021-05-26 12:55:43 - root - INFO: selected_scheduler = cosine_schedule_with_warmup
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [Label_mapping]
2021-05-26 12:55:43 - root - INFO: O = 0
2021-05-26 12:55:43 - root - INFO: B-PER = 1
2021-05-26 12:55:43 - root - INFO: I-PER = 2
2021-05-26 12:55:43 - root - INFO: B-ORG = 3
2021-05-26 12:55:43 - root - INFO: I-ORG = 4
2021-05-26 12:55:43 - root - INFO: B-LOC = 5
2021-05-26 12:55:43 - root - INFO: I-LOC = 6
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [Label]
2021-05-26 12:55:43 - root - INFO: label_type = PER,ORG,LOC
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [TF_model]
2021-05-26 12:55:43 - root - INFO: TF_Bert_pretrained_model = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/
2021-05-26 12:55:43 - root - INFO: TF_Bert_config = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_config.json
2021-05-26 12:55:43 - root - INFO: TF_Bert_checkpoint = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_model.ckpt
2021-05-26 12:55:43 - root - INFO: TF_Bert_vocab = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/vocab.txt
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [pytorch_model]
2021-05-26 12:55:43 - root - INFO: pytorch_Bert_pretrained_model = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/
2021-05-26 12:55:43 - root - INFO: pytorch_Bert_vocab = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/vocab.txt
2021-05-26 12:55:43 - root - INFO: pytorch_Bert_model = ./experiments/checkpoint_2021-05-25_07_45_37/
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [env]
2021-05-26 12:55:43 - root - INFO: devive = cuda
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [hyperparameter]
2021-05-26 12:55:43 - root - INFO: full_fine_tuning = True
2021-05-26 12:55:43 - root - INFO: batch_size = 16
2021-05-26 12:55:43 - root - INFO: epoch_num = 50
2021-05-26 12:55:43 - root - INFO: learning_rate = 3e-5
2021-05-26 12:55:43 - root - INFO: weight_decay = 0.01
2021-05-26 12:55:43 - root - INFO: clip_grad = 5
2021-05-26 12:55:43 - root - INFO: patience_num = 5
2021-05-26 12:55:43 - root - INFO: patience_value = 0.01
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - root - INFO: [training_setting]
2021-05-26 12:55:43 - root - INFO: saving_model_path = ./experiments/
2021-05-26 12:55:43 - root - INFO: 
2021-05-26 12:55:43 - __main__ - INFO: ===== import data =====
2021-05-26 12:56:07 - __main__ - INFO: ===== getting model =====
2021-05-26 12:56:12 - __main__ - INFO: ===== Starting Training =====
2021-05-26 12:56:12 - __main__ - INFO: ===== Epoch 1=====
2021-05-26 13:31:35 - __main__ - INFO: Epoch: 1, train loss: 323.0901026843843
2021-05-26 13:32:41 - __main__ - INFO: evaluate: {'full': 0.4003333333333333, 'label_score': {'PER': 0.7083333333333334, 'ORG': 0.6796666666666666, 'LOC': 0.5833333333333334}}
2021-05-26 13:32:41 - __main__ - INFO: best_eval_metric = 0.4003333333333333;current_num_patience = 0
2021-05-26 13:32:41 - __main__ - INFO: ===== Epoch 2=====
2021-05-26 14:08:13 - __main__ - INFO: Epoch: 2, train loss: 135.10830847458612
2021-05-26 14:09:18 - __main__ - INFO: evaluate: {'full': 0.527, 'label_score': {'PER': 0.844, 'ORG': 0.753, 'LOC': 0.693}}
2021-05-26 14:09:18 - __main__ - INFO: ***** Saving model *****
2021-05-26 14:09:20 - __main__ - INFO: best_eval_metric = 0.527;current_num_patience = 0
2021-05-26 14:09:20 - __main__ - INFO: ===== Epoch 3=====
2021-05-26 14:44:44 - __main__ - INFO: Epoch: 3, train loss: 83.62821469988141
2021-05-26 14:45:50 - __main__ - INFO: evaluate: {'full': 0.6116666666666667, 'label_score': {'PER': 0.8693333333333333, 'ORG': 0.822, 'LOC': 0.7536666666666666}}
2021-05-26 14:45:50 - __main__ - INFO: ***** Saving model *****
2021-05-26 14:45:53 - __main__ - INFO: best_eval_metric = 0.6116666666666667;current_num_patience = 0
2021-05-26 14:45:53 - __main__ - INFO: ===== Epoch 4=====
2021-05-26 15:21:31 - __main__ - INFO: Epoch: 4, train loss: 58.09929167247954
2021-05-26 15:22:37 - __main__ - INFO: evaluate: {'full': 0.7016666666666667, 'label_score': {'PER': 0.9166666666666666, 'ORG': 0.861, 'LOC': 0.82}}
2021-05-26 15:22:37 - __main__ - INFO: ***** Saving model *****
2021-05-26 15:22:39 - __main__ - INFO: best_eval_metric = 0.7016666666666667;current_num_patience = 0
2021-05-26 15:22:39 - __main__ - INFO: ===== Epoch 5=====
2021-05-26 15:58:12 - __main__ - INFO: Epoch: 5, train loss: 41.45310095505487
2021-05-26 15:59:17 - __main__ - INFO: evaluate: {'full': 0.7146666666666667, 'label_score': {'PER': 0.9196666666666666, 'ORG': 0.8523333333333334, 'LOC': 0.8386666666666667}}
2021-05-26 15:59:17 - __main__ - INFO: ***** Saving model *****
2021-05-26 15:59:19 - __main__ - INFO: best_eval_metric = 0.7146666666666667;current_num_patience = 0
2021-05-26 15:59:19 - __main__ - INFO: ===== Epoch 6=====
