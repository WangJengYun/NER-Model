[Dataset]
Dataset_path = data/traditional_ch/msra
is_cls_flag = True 
align_for_token_maxlen = False
cls_padding_idx = 0
token_padding_idx = 0 
label_padding_idx = -1
token_max_len = 128
sample_seed = 207

[model]
mode = testing
num_labels = 7
selected_model = NER_For_Bert
selected_optimizer = Transformer_Adaw
selected_scheduler = cosine_schedule_with_warmup

[Label_mapping]
O = 0
B-PER = 1
I-PER = 2
B-ORG = 3
I-ORG = 4
B-LOC = 5
I-LOC = 6

[Label]
label_type = PER,ORG,LOC

[TF_model] 
TF_Bert_pretrained_model = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/
TF_Bert_config = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_config.json
TF_Bert_checkpoint = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/bert_model.ckpt
TF_Bert_vocab = pretrained_bert_models/tensorflow/Bert_chinese_L-12_H-768_A-12/vocab.txt

[pytorch_model]
pytorch_Bert_pretrained_model = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/
pytorch_Bert_vocab = pretrained_bert_models/pytorch/Bert_chinese_L-12_H-768_A-12/vocab.txt
pytorch_Bert_model = ./experiments/checkpoint_2021-05-27_10_31_21/

[env]
devive = cuda

[hyperparameter]
full_fine_tuning = True
batch_size = 16
epoch_num = 50
learning_rate = 3e-5
weight_decay = 0.01
clip_grad = 5
patience_num = 5
patience_value = 0.01

[training_setting]
saving_model_path = ./experiments/